"""
This script will upload the output files generated by the harvester job.


Refer: https://bitbucket.org/eoncollective-code-repository/adept-ui-console/src/master/docs/integration-notes/index.md
for how to create nodes and relationships graph data with csv files.


## installation
pip3 install requests


Usage:

Make sure the following variables are updated towards the end of the script

cd docs/integration_notes/script 
python upload_harvester_output.py -i ~/Downloads/greenplum-data  -p 2 -t local -s dbt_to_adept
or 
python upload_harvester_output.py -i ~/Downloads/greenplum-data  -p 265 -t demo -s greenplum_to_adept
"""
import os
import requests
import pprint
from pathlib import Path
import argparse

"""
Output folder should have `graph_data` in the parent directory. Here is the reference of the output.

|-- graph_data
|   |-- nodes
|   |   |-- Table.csv
|   |   |-- Schema.csv
|   |-- relationships
|   |   |-- all-relationships.csv
|-- Models
|-- collibra-import.jar
|-- json
|   |-- TASKS_DETAILS.json
|-- log
|   |-- alllogs_goes_here.md
|-- query
|   |-- DEV_RAW
|   |   `-- TMC
|   |       |-- TASKS_DETAILS.sql



"""


# def trigger_graph_import(u, _node_file_ids, _relationship_file_ids):
#     print(f"triggering graph import job :: {u}")
#     payload = {"node_file_ids": _node_file_ids, "relationship_file_ids": _relationship_file_ids}
#     print("trigger_graph_import with payload", payload)
#     res = requests.post(u, data=payload, timeout=1000000)
#     print("response for import job:", res.text)
#     if res.status_code == 200:
#         print("========================================")
#         print("***** Graph imported successfully ******")
#     else:
#         print("Graph import FAILED, here is the response from server ")
#         with open("res.txt", "w") as f:
#             f.write(res.text)
#     print("===res.html", res.text)
#     return res.json()


def trigger_graph_import(u):
    print(f"triggering graph import job :: {u}")
    res = requests.get(u)
    print("response for import job:", res.text)
    if res.status_code == 200:
        print("========================================")
        print("***** Graph import initiated successfully ******")
    else:
        print("Graph import FAILED, here is the response from server ")
        with open("res.txt", "w") as f:
            f.write(res.text)
    print("===res.html", res.text)
    return res.json()


class HarvesterOutputUploader:

    def __init__(self, job_id, harverster_output_dir, _console_api_base_url):
        self.job_id = job_id
        self.output_path = harverster_output_dir
        self.console_api_base_url = _console_api_base_url

    @property
    def upload_files_api(self):
        return f"{self.console_api_base_url}/api/job/stage/{self.job_id}/files"

    def check_if_path_should_be_skipped(self, path, skip_folders):
        if skip_folders is None:
            return False
        for skip_folder in skip_folders:
            # if path.endswith(skip_folder):
            if skip_folder in path:
                return True
        return False

    def walk_through_path(self, p, skip_folders=None):
        file_paths = []
        for path, subdirs, files in os.walk(p):
            should_be_skipped = self.check_if_path_should_be_skipped(path, skip_folders)
            if should_be_skipped is not True:
                for name in files:
                    if name not in [".DS_Store"]:
                        file_paths.append(os.path.join(path, name))
        return file_paths

    def get_file_path_within_job(self, full_file_path):
        path_with_in_job = full_file_path.replace(self.output_path, "").lstrip("/").lstrip(
            "\\")  # this is with files name
        _ = Path(path_with_in_job)  # .strip("/").strip("\\")
        return _.parent

    def upload_file(self, full_file_path):
        print(f"Uploading {full_file_path}")
        file_path_in_job = self.get_file_path_within_job(full_file_path)
        files = {"file": open(full_file_path, 'rb'), }
        print("file_path_in_job", file_path_in_job)
        res = requests.post(self.upload_files_api, files=files, data={'file_path': file_path_in_job}, )
        return res.json()

    def upload_files_batch(self, files):
        raise NotImplementedError()

    def upload_files(self, output_path, skip_folders=None):
        # this will return the list of the files uploaded
        print(f"Uploading from {output_path}")
        file_paths = self.walk_through_path(output_path, skip_folders=skip_folders)
        print(f"Found {len(file_paths)} files in the path :: {output_path}")
        print("Here are the files to upload --")
        pprint.pprint(file_paths)
        uploaded_filepaths = []
        for file_path in file_paths:
            try:
                _ = self.upload_file(file_path)
                print(_)
                uploaded_filepaths.append(_)
            except Exception as e:
                # raise e
                print(f"Failed to upload file {file_path} with error:: {e.__str__()}")
        return uploaded_filepaths


def upload_data(_console_api_base_url, harverster_output_dir, job_id):
    """
    """
    # for demo  
 

    # graph_data_dir = os.path.join(harverster_output_dir, os.path.join("output", os.path.join(str(job_id), "graph_data")))
    graph_data_dir = harverster_output_dir #os.path.join(harverster_output_dir,   "graph_data")

    print("graph_data_dir", graph_data_dir)
 

    uploader = HarvesterOutputUploader(job_id, harverster_output_dir, _console_api_base_url)
    # this will upload all files
    all_files = uploader.upload_files(harverster_output_dir)

    print("============")
    print(f"Created {all_files.__len__()} all_files {harverster_output_dir}")
    pprint.pprint(all_files)


def create_dummy_job(_console_api_base_url, project_id, STAGE_NAME):
    """
    """
    # create job in a project
    # job_id = 1
    url = f"{_console_api_base_url}/api/project/{project_id}/create-dummy-job"
    print("======url" , url)
    res = requests.get(url)
    print("response for import job:", res.status_code)
    data = res.json()
    job_id =data['dummy_job_id']
    stages = data['stages']
    stages_id_map = data['stages_id_map']
    workspace_slug = data['workspace_slug']

    return job_id, stages, stages_id_map, workspace_slug

def get_defaults(_console_type):
    """
    """
    if _console_type == "demo":
        CONSOLE_API_BASE_URL = "http://demo.adept.eoncollective.com"
        GRAPH_IMPORTER_BASE_URL = "http://54.166.82.211:5005"
    else:
        CONSOLE_API_BASE_URL = "http://localhost:8001"
        GRAPH_IMPORTER_BASE_URL = "http://localhost:5005"

    return CONSOLE_API_BASE_URL, GRAPH_IMPORTER_BASE_URL


if __name__ == "__main__":
    """
    Main function: driver for application upload_harvester_output.py
    """
    parser = argparse.ArgumentParser(
                    prog='ADEPT Upload Harvester Output',
                    description='''This program is used to test uploading to the ADEPT Console. This script will upload the output files generated by the harvester job.
Refer: https://bitbucket.org/eoncollective-code-repository/adept-ui-console/src/master/docs/integration-notes/index.md
for how to create nodes and relationships graph data with csv files.''',
                    epilog='ADEPT utilities')
    parser.add_argument('-i','--input',
                    default=None,
                    required=True,
                    help='Harvester output directory')
    parser.add_argument('-t','--type',
                        default='local',
                        choices=['demo', 'local'],
                        required=True,
                        help='Type of target to use for this dumy job run')
    parser.add_argument('-p','--project-id',
                        type=int,
                        required=True,
                        default=None,
                        help='project id to upload to in ADEPT')
    parser.add_argument('-s','--stage',
                        default=None,
                        # required=True,
                        help='Stage name to which output should be pushed.')

    parser.add_argument('-v','--version', action='version', version='%(prog)s 1.0')

    args = parser.parse_args()
    CONSOLE_TYPE = args.type

    HARVESTER_OUTPUT_DIR = args.input  # << CHANGE THIS FOLDER to the harvester output folder.
    # HARVESTER_OUTPUT_DIR = "/Users/ravi.merugu/Projects/eon/adept-ui-console/docs/integration-notes/output"
    PROJECT_ID = args.project_id
    STAGE_NAME = args.stage
    CONSOLE_API_BASE_URL, GRAPH_IMPORTER_BASE_URL = get_defaults(CONSOLE_TYPE)
    print (CONSOLE_API_BASE_URL)

#     # for localhost
    JOB_ID, JOB_STAGES, stages_id_map, workspace_slug = create_dummy_job(CONSOLE_API_BASE_URL, PROJECT_ID,  STAGE_NAME)
    JOB_ID = int(JOB_ID)
    if STAGE_NAME not in JOB_STAGES:
        raise Exception(f"{STAGE_NAME} not found in available stages for the job which are : {JOB_STAGES}")
    
    job_stage_id = stages_id_map[STAGE_NAME]

    upload_data(CONSOLE_API_BASE_URL, HARVESTER_OUTPUT_DIR, job_stage_id)
    
    
    GRAPH_IMPORT_API_URL = f"{GRAPH_IMPORTER_BASE_URL}/api/job/stage/{job_stage_id}/import-graph"

    # GRAPH_IMPORT_API_URL = f"{CONSOLE_API_BASE_URL}/api/job/stage/{job_stage_id}/v1/import-graph"
    # trigger_graph_import(GRAPH_IMPORT_API_URL, node_file_ids, relationship_file_ids)
    trigger_graph_import(GRAPH_IMPORT_API_URL)
    print("========================================")

    if CONSOLE_TYPE == "local":
        job_url = f"http://localhost:8001/o/{workspace_slug}/project/{PROJECT_ID}/job/{JOB_ID}/stage/{job_stage_id}"
    else:
        job_url = f"http://demo.adept.eoncollective.com/o/{workspace_slug}/project/{PROJECT_ID}/job/{JOB_ID}/stage/{job_stage_id}"

    msg = f"""
You can use the cypher query below to get the nodes created by the job :{JOB_ID}    
    
MATCH (n) where n.job_id = "{JOB_ID}" return n
or 
MATCH (n:ADEPTJob) where n.job_id = "{JOB_ID}" return n
    
Check uploaded files at {job_url}
    """
    print(msg)
